{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argoverse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argoverse.map_representation.map_api import ArgoverseMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7241e-01,  7.3474e-01,  6.7025e-01, -2.5457e+00],\n",
       "         [ 1.2746e-01, -9.7692e-01, -1.0448e-01, -1.5189e-01],\n",
       "         [ 7.5036e-01,  3.1237e-04, -1.0763e+00,  1.8481e-01]],\n",
       "\n",
       "        [[ 9.3107e-01, -1.1606e-01, -6.0820e-01, -8.8039e-01],\n",
       "         [-1.7798e+00, -1.3101e+00, -1.4513e+00, -5.2073e-01],\n",
       "         [ 1.6815e+00,  7.3596e-01,  1.5130e+00, -8.7353e-01]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(2,3,4)\n",
    "a.shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1456],\n",
       "         [ 0.1413],\n",
       "         [ 1.8430]],\n",
       "\n",
       "        [[ 1.4236],\n",
       "         [ 1.9804],\n",
       "         [-2.3153]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.randn(2,3,1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5105e-02,  1.0698e-01,  9.7594e-02, -3.7067e-01],\n",
       "         [ 1.8005e-02, -1.3800e-01, -1.4758e-02, -2.1456e-02],\n",
       "         [ 1.3829e+00,  5.7571e-04, -1.9836e+00,  3.4060e-01]],\n",
       "\n",
       "        [[ 1.3255e+00, -1.6523e-01, -8.6583e-01, -1.2533e+00],\n",
       "         [-3.5246e+00, -2.5945e+00, -2.8741e+00, -1.0312e+00],\n",
       "         [-3.8931e+00, -1.7040e+00, -3.5031e+00,  2.0225e+00]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b\n",
    "a.mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "b: tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "c: tensor([[ 2,  6, 12],\n",
      "        [20, 30, 42]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a=torch.tensor([[1,2,3],[4,5,6]])\n",
    "b=torch.tensor([[2,3,4],[5,6,7]])\n",
    "c=torch.mul(a,b)\n",
    "print('a:',a)\n",
    "print('b:',b)\n",
    "print('c:',c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "b=a.sum(dim=-1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,c=a.chunk(2,dim=-1)\n",
    "b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(4)\n",
    "a.shape\n",
    "b,c=a.chunk(2,dim=-1)\n",
    "b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3]\n",
    "a[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.6.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,4]\n",
    "b[b[1].isin(a)]\n",
    "# b[1].isin(a)\n",
    "# b[1]\n",
    "type(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/mnt/home/husiyuan/code/argo_my/ARGO1/data/train/data/1.csv\") # Dataframe格式所以df\n",
    "\n",
    "timestamps=list(np.sort(df['TIMESTAMP'].unique())) #list:一辆车被记录的所有帧，df['TIMESTAMP']是Series, Hash table-based unique去重以后unique()返回ndarray\n",
    "# timestamps\n",
    "historical_timestamps=timestamps[:20] # ?why 可改 只取前20帧从0-19\n",
    "# historical_timestamps\n",
    "historical_df=df[df['TIMESTAMP'].isin(historical_timestamps)]   # 返回boolean series，然后选择true的行\n",
    "# historical_df\n",
    "\n",
    "\n",
    "actor_ids=list(historical_df['TRACK_ID'].unique()) # 前20帧的无重复track_id\n",
    "# print(len(actor_ids))\n",
    "\n",
    "df=df[df['TRACK_ID'].isin(actor_ids)] # 是series\n",
    "print(historical_df.shape)\n",
    "print(df.shape)\n",
    "# num_nodes=len(actor_ids)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for a,b in df.groupby('TRACK_ID'):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_df = df[df['OBJECT_TYPE'] == 'AV'].iloc\n",
    "av_index = actor_ids.index(av_df[0]['TRACK_ID'])\n",
    "av_df.__sizeof__() # 56 因为50帧+6个属性\n",
    "av_df[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CITY_NAME'].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.6.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition=torch.zeros(3,2)\n",
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(condition[:,1].shape)\n",
    "print(condition[:,1].unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "condition[:,1:].unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "print(list(permutations(range(3),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(('134'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.6.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.splitext(\"../../../data/forecasting_sample_v1.1/forecasting_sample/data/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(2,3,4)\n",
    "a.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.6.21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Define a LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-21 06:02:48,324] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from torch import optim,nn,utils,Tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=nn.Sequential(nn.Linear(28*28,64),nn.ReLU(),nn.Linear(64,3))\n",
    "decoder=nn.Sequential(nn.Linear(3,64),nn.ReLU(),nn.Linear(64,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule 把train包进来了！！\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)  # 太牛逼了！！！\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "# init the autoencoder 就是model+train+optimizer\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Define a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Train the model\n",
    "\n",
    "The Lightning Trainer automates 40+ tricks including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/mnt/home/husiyuan/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "2023-06-21 05:48:16.679108: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 05:48:16.721067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 05:48:17.514369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/mnt/home/husiyuan/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6644da0a7b4491856069cc09a11eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[ 1.5403e+30,  1.2544e+32, -1.8449e+32],\n",
      "        [-9.4261e+30,  1.1482e+31,  1.3236e+31],\n",
      "        [ 9.9755e-02,  3.2822e-02, -1.1311e-01],\n",
      "        [ 9.6513e-02,  3.0934e-02, -1.1387e-01]], grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=99.ckpt\"\n",
    "autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# choose your trained nn.Module\n",
    "encoder = autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = Tensor(4, 28 * 28)\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 05:55:47.521858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-21 05:55:48.276855: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /mnt/home/husiyuan/.local/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "I0621 05:55:48.835489 140234744825600 plugin.py:429] Monitor runs begin\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.12.2 at http://localhost:6008/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir . --port=6008"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Supercharge training\n",
    "\n",
    "Enable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/husiyuan/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:849: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(strategy=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `strategy=\"ddp_spawn\"` for you.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='deepspeed')` or `Trainer(accelerator='deepspeed')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39mpl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      3\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      4\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m  )\n\u001b[1;32m      7\u001b[0m \u001b[39m# train 1TB+ parameter models with Deepspeed/fsdp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# 分布式并行-4——PyTorch数据并行DP、DDP、FSDP原理\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(\n\u001b[1;32m     10\u001b[0m     devices\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdeepspeed_stage_2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     precision\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m\n\u001b[1;32m     14\u001b[0m  )\n\u001b[1;32m     16\u001b[0m \u001b[39m# 20+ helpful flags for rapid idea iteration\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     18\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     19\u001b[0m     min_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     20\u001b[0m     overfit_batches\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     21\u001b[0m  )\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py:38\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     37\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:431\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 431\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    432\u001b[0m     num_processes,\n\u001b[1;32m    433\u001b[0m     devices,\n\u001b[1;32m    434\u001b[0m     tpu_cores,\n\u001b[1;32m    435\u001b[0m     ipus,\n\u001b[1;32m    436\u001b[0m     accelerator,\n\u001b[1;32m    437\u001b[0m     strategy,\n\u001b[1;32m    438\u001b[0m     gpus,\n\u001b[1;32m    439\u001b[0m     gpu_ids,\n\u001b[1;32m    440\u001b[0m     num_nodes,\n\u001b[1;32m    441\u001b[0m     sync_batchnorm,\n\u001b[1;32m    442\u001b[0m     benchmark,\n\u001b[1;32m    443\u001b[0m     replace_sampler_ddp,\n\u001b[1;32m    444\u001b[0m     deterministic,\n\u001b[1;32m    445\u001b[0m     precision,\n\u001b[1;32m    446\u001b[0m     amp_backend,\n\u001b[1;32m    447\u001b[0m     amp_level,\n\u001b[1;32m    448\u001b[0m     plugins,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:164\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_accelerator_type()\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_training_type_plugin()\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_distributed_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:311\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_training_type_plugin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_type_plugin \u001b[39m=\u001b[39m TrainingTypePluginsRegistry\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_distributed_mode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy)\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy, TrainingTypePlugin):\n\u001b[1;32m    313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_type_plugin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:902\u001b[0m, in \u001b[0;36mAcceleratorConnector.set_distributed_mode\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# finished configuring self._distrib_type, check ipython environment\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_interactive_compatibility()\n\u001b[1;32m    904\u001b[0m \u001b[39m# for DDP overwrite nb processes by requested GPUs\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_type \u001b[39m==\u001b[39m DeviceType\u001b[39m.\u001b[39mGPU \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39min\u001b[39;00m (\n\u001b[1;32m    906\u001b[0m     DistributedType\u001b[39m.\u001b[39mDDP,\n\u001b[1;32m    907\u001b[0m     DistributedType\u001b[39m.\u001b[39mDDP_SPAWN,\n\u001b[1;32m    908\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/hivt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:943\u001b[0m, in \u001b[0;36mAcceleratorConnector.check_interactive_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_INTERACTIVE\n\u001b[1;32m    942\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type\u001b[39m.\u001b[39mis_interactive_compatible():\n\u001b[0;32m--> 943\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    944\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer(strategy=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type\u001b[39m.\u001b[39mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m)` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer(accelerator=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type\u001b[39m.\u001b[39mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m)` is not compatible with an interactive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    946\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m environment. Run your code as a script, or choose one of the compatible backends:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    947\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(DistributedType\u001b[39m.\u001b[39minteractive_compatible_types())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m creation inside the worker function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m     )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='deepspeed')` or `Trainer(accelerator='deepspeed')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "# train on 4 GPUs\n",
    "trainer =pl.Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    " )\n",
    "\n",
    "# train 1TB+ parameter models with Deepspeed/fsdp\n",
    "# 分布式并行-4——PyTorch数据并行DP、DDP、FSDP原理\n",
    "trainer = pl.Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"deepspeed_stage_2\",\n",
    "    precision=16\n",
    " )\n",
    "\n",
    "# 20+ helpful flags for rapid idea iteration\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=1\n",
    " )\n",
    "\n",
    "# # access the latest state of the art techniques\n",
    "# trainer = pl.Trainer(callbacks=[StochasticWeightAveraging(...)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.6.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0442,  0.0721, -0.3050,  0.4946, -1.3777],\n",
       "        [ 0.2024, -1.4048,  0.9374, -1.5055, -0.0323],\n",
       "        [-0.2134,  2.1043,  0.7709,  0.2246, -0.3878]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.Tensor(3, 5)\n",
    "nn.init.normal_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(a:Optional[torch.Tensor]):\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=value([torch.randn(2,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give(a):\n",
    "    print(a)\n",
    "    print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m give(\u001b[39m*\u001b[39mb\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "give(*b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (2754346888.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[59], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    *b\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/7/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (2881932770.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    a=np.array(([[[1,2],[3,4],[5,6]],[[3,4],[5,6]],[7,8])))\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2],[3,4]])\n",
    "a.shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39mcumsum(\u001b[39m-\u001b[39ma[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "np.cumsum(-a[0, 1::-1, :], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.7.19 debugpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiVT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
